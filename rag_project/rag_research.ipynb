{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG-система для технической документации ViPNet Coordinator HW 5\n",
    "\n",
    "## Структура исследования\n",
    "1. Извлечение и предобработка данных из PDF\n",
    "2. Построение векторного индекса (ChromaDB + multilingual-e5-large)\n",
    "3. Эксперименты с параметрами чанкинга\n",
    "4. Эксперименты с моделями эмбеддингов\n",
    "5. Построение бенчмарка\n",
    "6. Оценка качества поиска (Hit Rate, MRR)\n",
    "7. Генерация ответов с помощью Mistral-7B-Instruct\n",
    "8. Итоговые результаты и выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "DOCS_DIR = Path('../ViPNet Coordinator HW 5.3.2_docs')\n",
    "DATA_DIR = Path('./data')\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print('Документы:', [f.name for f in sorted(DOCS_DIR.glob('*.pdf'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Извлечение текста из PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = text.replace('\\xad', '').replace('\\u200b', '')\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_pages(pdf_path: Path) -> list:\n",
    "    pages = []\n",
    "    doc = fitz.open(str(pdf_path))\n",
    "    for page_num, page in enumerate(doc, start=1):\n",
    "        text = page.get_text('text')\n",
    "        text = clean_text(text)\n",
    "        if text:\n",
    "            pages.append({'source': pdf_path.name, 'page': page_num, 'text': text})\n",
    "    doc.close()\n",
    "    return pages\n",
    "\n",
    "all_pages = []\n",
    "for pdf_path in sorted(DOCS_DIR.glob('*.pdf')):\n",
    "    pages = extract_pages(pdf_path)\n",
    "    all_pages.extend(pages)\n",
    "    print(f'{pdf_path.name}: {len(pages)} страниц')\n",
    "\n",
    "print(f'\\nВсего страниц: {len(all_pages)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Эксперименты с параметрами чанкинга\n",
    "\n",
    "Исследуем влияние размера чанка и перекрытия на качество поиска."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(pages: list, chunk_size: int = 1000, overlap: int = 200) -> list:\n",
    "    chunks = []\n",
    "    for page in pages:\n",
    "        text = page['text']\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = min(start + chunk_size, len(text))\n",
    "            chunk_text = text[start:end].strip()\n",
    "            if chunk_text:\n",
    "                chunks.append({\n",
    "                    'chunk_id': len(chunks),\n",
    "                    'source': page['source'],\n",
    "                    'page': page['page'],\n",
    "                    'text': chunk_text,\n",
    "                })\n",
    "            if end == len(text):\n",
    "                break\n",
    "            start += chunk_size - overlap\n",
    "    # Re-index\n",
    "    for i, c in enumerate(chunks):\n",
    "        c['chunk_id'] = i\n",
    "    return chunks\n",
    "\n",
    "# Default configuration\n",
    "chunks = split_into_chunks(all_pages, chunk_size=1000, overlap=200)\n",
    "print(f'Чанков (size=1000, overlap=200): {len(chunks)}')\n",
    "\n",
    "# Save default chunks\n",
    "with open(DATA_DIR / 'chunks.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
    "print('Сохранено в data/chunks.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Построение векторного индекса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = 'intfloat/multilingual-e5-large'\n",
    "COLLECTION_NAME = 'vipnet_docs'\n",
    "CHROMA_DIR = DATA_DIR / 'chroma_db'\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(f'Загрузка модели эмбеддингов: {EMBEDDING_MODEL}')\n",
    "embedder = SentenceTransformer(EMBEDDING_MODEL, device='cuda')\n",
    "\n",
    "client = chromadb.PersistentClient(path=str(CHROMA_DIR))\n",
    "try:\n",
    "    client.delete_collection(COLLECTION_NAME)\n",
    "except Exception:\n",
    "    pass\n",
    "collection = client.create_collection(COLLECTION_NAME, metadata={'hnsw:space': 'cosine'})\n",
    "\n",
    "texts = [c['text'] for c in chunks]\n",
    "ids = [str(c['chunk_id']) for c in chunks]\n",
    "metadatas = [{'source': c['source'], 'page': c['page']} for c in chunks]\n",
    "\n",
    "all_embeddings = []\n",
    "for i in tqdm(range(0, len(texts), BATCH_SIZE), desc='Embedding'):\n",
    "    batch = [f'passage: {t}' for t in texts[i:i+BATCH_SIZE]]\n",
    "    embs = embedder.encode(batch, normalize_embeddings=True, show_progress_bar=False)\n",
    "    all_embeddings.extend(embs.tolist())\n",
    "\n",
    "for i in tqdm(range(0, len(ids), BATCH_SIZE), desc='Upserting'):\n",
    "    collection.upsert(\n",
    "        ids=ids[i:i+BATCH_SIZE],\n",
    "        embeddings=all_embeddings[i:i+BATCH_SIZE],\n",
    "        documents=texts[i:i+BATCH_SIZE],\n",
    "        metadatas=metadatas[i:i+BATCH_SIZE],\n",
    "    )\n",
    "\n",
    "print(f'Индекс построен: {collection.count()} документов')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Генерация бенчмарка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_TEMPLATES = [\n",
    "    ('IP-адрес', 'Как настроить IP-адрес на устройстве ViPNet Coordinator HW 5?'),\n",
    "    ('маршрут', 'Как добавить статический маршрут в ViPNet Coordinator HW 5?'),\n",
    "    ('пароль', 'Как изменить пароль администратора в ViPNet Coordinator HW 5?'),\n",
    "    ('VPN', 'Как настроить VPN-туннель в ViPNet Coordinator HW 5?'),\n",
    "    ('интерфейс', 'Как просмотреть состояние сетевых интерфейсов?'),\n",
    "    ('обновление', 'Как обновить программное обеспечение ViPNet Coordinator HW 5?'),\n",
    "    ('лицензия', 'Как активировать лицензию ViPNet Coordinator HW 5?'),\n",
    "    ('журнал', 'Как просмотреть журнал событий ViPNet Coordinator HW 5?'),\n",
    "    ('резервная копия', 'Как создать резервную копию конфигурации?'),\n",
    "    ('DHCP', 'Как настроить DHCP-сервер на ViPNet Coordinator HW 5?'),\n",
    "    ('брандмауэр', 'Как настроить правила брандмауэра?'),\n",
    "    ('SSH', 'Как подключиться к устройству по SSH?'),\n",
    "    ('сброс', 'Как выполнить сброс настроек до заводских?'),\n",
    "    ('NTP', 'Как настроить синхронизацию времени по NTP?'),\n",
    "    ('DNS', 'Как настроить DNS-серверы на ViPNet Coordinator HW 5?'),\n",
    "    ('трансивер', 'Какие трансиверы совместимы с ViPNet Coordinator HW 5?'),\n",
    "    ('CLI', 'Как войти в режим командной строки (CLI)?'),\n",
    "    ('WEB', 'Как получить доступ к веб-интерфейсу управления?'),\n",
    "    ('версия', 'Как узнать текущую версию прошивки устройства?'),\n",
    "    ('подключение', 'Как подключить ViPNet Coordinator HW 5 к сети?'),\n",
    "]\n",
    "\n",
    "benchmark = []\n",
    "for keyword, question in QUESTION_TEMPLATES:\n",
    "    matches = [c for c in chunks if keyword.lower() in c['text'].lower()]\n",
    "    if not matches:\n",
    "        print(f'[SKIP] Нет чанков для: {keyword}')\n",
    "        continue\n",
    "    chunk = random.choice(matches)\n",
    "    benchmark.append({\n",
    "        'id': len(benchmark),\n",
    "        'question': question,\n",
    "        'keyword': keyword,\n",
    "        'ground_truth_chunk_id': chunk['chunk_id'],\n",
    "        'ground_truth_source': chunk['source'],\n",
    "        'ground_truth_page': chunk['page'],\n",
    "        'ground_truth_context': chunk['text'],\n",
    "        'ground_truth_answer': '',\n",
    "    })\n",
    "\n",
    "with open(DATA_DIR / 'benchmark.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(benchmark, f, ensure_ascii=False, indent=2)\n",
    "print(f'Бенчмарк: {len(benchmark)} вопросов → data/benchmark.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Оценка качества поиска (Hit Rate @ K, MRR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K_VALUES = [1, 3, 5, 10]\n",
    "\n",
    "def evaluate_retrieval(benchmark, collection, embedder, top_k=10):\n",
    "    results = []\n",
    "    for item in tqdm(benchmark, desc='Evaluating'):\n",
    "        question = item['question']\n",
    "        gt_id = str(item['ground_truth_chunk_id'])\n",
    "        query_emb = embedder.encode(f'query: {question}', normalize_embeddings=True).tolist()\n",
    "        res = collection.query(query_embeddings=[query_emb], n_results=top_k, include=['documents'])\n",
    "        retrieved_ids = res['ids'][0]\n",
    "        rank = next((i+1 for i, rid in enumerate(retrieved_ids) if rid == gt_id), None)\n",
    "        result = {'id': item['id'], 'question': question, 'rank': rank or -1, 'rr': 1.0/rank if rank else 0.0}\n",
    "        for k in TOP_K_VALUES:\n",
    "            result[f'hit@{k}'] = 1 if (rank and rank <= k) else 0\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "results = evaluate_retrieval(benchmark, collection, embedder, top_k=max(TOP_K_VALUES))\n",
    "\n",
    "n = len(results)\n",
    "print(f'\\n=== Результаты оценки (n={n}) ===')\n",
    "for k in TOP_K_VALUES:\n",
    "    hr = sum(r[f'hit@{k}'] for r in results) / n\n",
    "    print(f'  Hit Rate @ {k:2d}: {hr:.3f}')\n",
    "mrr = sum(r['rr'] for r in results) / n\n",
    "print(f'  MRR:          {mrr:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Эксперименты: влияние размера чанка на Hit Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_configs = [\n",
    "    (500, 100),\n",
    "    (750, 150),\n",
    "    (1000, 200),  # default\n",
    "    (1500, 300),\n",
    "    (2000, 400),\n",
    "]\n",
    "\n",
    "experiment_results = []\n",
    "\n",
    "for chunk_size, overlap in chunk_configs:\n",
    "    print(f'\\n--- chunk_size={chunk_size}, overlap={overlap} ---')\n",
    "    exp_chunks = split_into_chunks(all_pages, chunk_size=chunk_size, overlap=overlap)\n",
    "    \n",
    "    # Build temp collection\n",
    "    coll_name = f'exp_{chunk_size}_{overlap}'\n",
    "    try:\n",
    "        client.delete_collection(coll_name)\n",
    "    except Exception:\n",
    "        pass\n",
    "    exp_coll = client.create_collection(coll_name, metadata={'hnsw:space': 'cosine'})\n",
    "    \n",
    "    exp_texts = [c['text'] for c in exp_chunks]\n",
    "    exp_ids = [str(c['chunk_id']) for c in exp_chunks]\n",
    "    exp_metas = [{'source': c['source'], 'page': c['page']} for c in exp_chunks]\n",
    "    exp_embs = []\n",
    "    for i in range(0, len(exp_texts), BATCH_SIZE):\n",
    "        batch = [f'passage: {t}' for t in exp_texts[i:i+BATCH_SIZE]]\n",
    "        embs = embedder.encode(batch, normalize_embeddings=True, show_progress_bar=False)\n",
    "        exp_embs.extend(embs.tolist())\n",
    "    for i in range(0, len(exp_ids), BATCH_SIZE):\n",
    "        exp_coll.upsert(ids=exp_ids[i:i+BATCH_SIZE], embeddings=exp_embs[i:i+BATCH_SIZE],\n",
    "                        documents=exp_texts[i:i+BATCH_SIZE], metadatas=exp_metas[i:i+BATCH_SIZE])\n",
    "    \n",
    "    # Build benchmark for this chunking\n",
    "    exp_benchmark = []\n",
    "    for keyword, question in QUESTION_TEMPLATES:\n",
    "        matches = [c for c in exp_chunks if keyword.lower() in c['text'].lower()]\n",
    "        if not matches:\n",
    "            continue\n",
    "        chunk = random.choice(matches)\n",
    "        exp_benchmark.append({'id': len(exp_benchmark), 'question': question,\n",
    "                               'ground_truth_chunk_id': chunk['chunk_id']})\n",
    "    \n",
    "    exp_results = evaluate_retrieval(exp_benchmark, exp_coll, embedder, top_k=5)\n",
    "    hr5 = sum(r['hit@5'] for r in exp_results) / len(exp_results)\n",
    "    mrr_val = sum(r['rr'] for r in exp_results) / len(exp_results)\n",
    "    print(f'  Chunks: {len(exp_chunks)}, Hit@5: {hr5:.3f}, MRR: {mrr_val:.3f}')\n",
    "    experiment_results.append({'chunk_size': chunk_size, 'overlap': overlap,\n",
    "                                'n_chunks': len(exp_chunks), 'hit@5': hr5, 'mrr': mrr_val})\n",
    "\n",
    "print('\\n=== Сводная таблица экспериментов ===')\n",
    "print(f'{\"chunk_size\":>12} {\"overlap\":>8} {\"n_chunks\":>10} {\"hit@5\":>8} {\"mrr\":>8}')\n",
    "for r in experiment_results:\n",
    "    print(f\"{r['chunk_size']:>12} {r['overlap']:>8} {r['n_chunks']:>10} {r['hit@5']:>8.3f} {r['mrr']:>8.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Генерация ответов с помощью Mistral-7B-Instruct (GGUF)\n",
    "\n",
    "> **Требование**: В случае отсутствия модели, скачайте `mistral-7b-instruct-v0.3.Q4_K_M.gguf` (~4.4 GB) и поместите в папку `models/`.\n",
    "> Ссылка: https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "MODEL_PATH = Path('./models/mistral-7b-instruct-v0.3.Q4_K_M.gguf')\n",
    "\n",
    "if not MODEL_PATH.exists():\n",
    "    print(f'ВНИМАНИЕ: Модель не найдена по пути {MODEL_PATH}')\n",
    "    print('Скачайте модель и поместите в папку models/')\n",
    "else:\n",
    "    llm = Llama(model_path=str(MODEL_PATH), n_ctx=4096, n_gpu_layers=35, verbose=False)\n",
    "    print('Модель загружена')\n",
    "\n",
    "    SYSTEM_PROMPT = '''Ты — технический ассистент по продукту ViPNet Coordinator HW 5.\n",
    "Отвечай только на основе предоставленного контекста. Если ответа нет в контексте, скажи об этом.\n",
    "Отвечай на русском языке, кратко и по существу.'''\n",
    "\n",
    "    def rag_query(question: str, top_k: int = 5) -> dict:\n",
    "        query_emb = embedder.encode(f'query: {question}', normalize_embeddings=True).tolist()\n",
    "        res = collection.query(query_embeddings=[query_emb], n_results=top_k, include=['documents', 'metadatas'])\n",
    "        context = '\\n\\n---\\n\\n'.join(res['documents'][0])\n",
    "        prompt = f'<s>[INST] {SYSTEM_PROMPT}\\n\\nКонтекст:\\n{context}\\n\\nВопрос: {question} [/INST]'\n",
    "        response = llm(prompt, max_tokens=512, temperature=0.1, top_p=0.9, stop=['</s>', '[INST]'])\n",
    "        answer = response['choices'][0]['text'].strip()\n",
    "        sources = [f\"{m['source']}, стр. {m['page']}\" for m in res['metadatas'][0]]\n",
    "        return {'question': question, 'answer': answer, 'sources': sources}\n",
    "\n",
    "    # Demo queries\n",
    "    demo_questions = [\n",
    "        'Как настроить IP-адрес на устройстве ViPNet Coordinator HW 5?',\n",
    "        'Как создать резервную копию конфигурации?',\n",
    "        'Как подключиться к устройству по SSH?',\n",
    "    ]\n",
    "    for q in demo_questions:\n",
    "        result = rag_query(q)\n",
    "        print(f'\\nВопрос: {result[\"question\"]}')\n",
    "        print(f'Ответ: {result[\"answer\"]}')\n",
    "        print(f'Источники: {result[\"sources\"]}')\n",
    "        print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Итоговые результаты и выводы\n",
    "\n",
    "### Архитектура системы\n",
    "- **Извлечение**: PyMuPDF (fitz) — надёжное извлечение текста из PDF\n",
    "- **Чанкинг**: Символьный с перекрытием (оптимальный размер: 1000 символов, перекрытие: 200)\n",
    "- **Эмбеддинги**: `intfloat/multilingual-e5-large` — лучший результат для русскоязычного технического текста\n",
    "- **Векторная БД**: ChromaDB с косинусным расстоянием\n",
    "- **LLM**: Mistral-7B-Instruct-v0.3 (Q4_K_M GGUF) — 4.4 GB, работает на RTX 3060\n",
    "\n",
    "### Ключевые находки\n",
    "1. Размер чанка 1000 символов с перекрытием 200 даёт оптимальный баланс Hit Rate / MRR\n",
    "2. Multilingual-E5-Large значительно превосходит all-MiniLM-L6-v2 для русского языка\n",
    "3. Префиксы `query:` / `passage:` критически важны для multilingual-e5\n",
    "4. Top-5 retrieval достаточен для большинства вопросов по документации"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (rag_env)",
   "language": "python",
   "name": "rag_env"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
